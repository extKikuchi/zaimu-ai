#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Streamlit Cloudå¯¾å¿œç‰ˆ Excel ãƒ‡ãƒ¼ã‚¿é›†è¨ˆã‚·ã‚¹ãƒ†ãƒ 
AWS Lambdaé€£æºï¼ˆS3ãƒã‚±ãƒƒãƒˆè‡ªå‹•ä½œæˆå¯¾å¿œï¼‰
"""

import streamlit as st
try:
    import pandas as pd
except ImportError as e:
    st.error(f"pandasã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
    st.stop()
import boto3
import json
import tempfile
import zipfile
from pathlib import Path
import io
import time
from datetime import datetime
import uuid
import os

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="ğŸ“Š Excel ãƒ‡ãƒ¼ã‚¿é›†è¨ˆã‚·ã‚¹ãƒ†ãƒ ",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# AWSè¨­å®š
@st.cache_data
def get_aws_config():
    """AWSè¨­å®šã‚’å–å¾—"""
    
    # æ—¢å­˜ã®excel_aggregatorãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‹ã‚‰aws_config.jsonã‚’èª­ã¿è¾¼ã¿
    try:
        import os
        # è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®excel_aggregatorãƒ•ã‚©ãƒ«ãƒ€ã‚’æ¢ã—ã¦aws_config.jsonã‚’å–å¾—
        current_dir = os.path.dirname(os.path.abspath(__file__))
        excel_aggregator_path = os.path.join(os.path.dirname(os.path.dirname(current_dir)), 'excel_aggregator')
        config_path = os.path.join(excel_aggregator_path, 'aws_config.json')
        
        if os.path.exists(config_path):
            with open(config_path, 'r') as f:
                local_config = json.load(f)
            
            config = {
                'region': local_config.get('Region', st.secrets.get('AWS_DEFAULT_REGION', 'ap-northeast-1')),
                'lambda_function_name': local_config.get('FunctionName', st.secrets.get('LAMBDA_FUNCTION_NAME', 'excel-data-aggregator')),
                'bucket_name': local_config.get('BucketName', st.secrets.get('S3_BUCKET_NAME', 'excel-aggregator-20250714-095126'))
            }
            st.sidebar.success(f"âœ… aws_config.jsonã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
            st.sidebar.write(f"  â€¢ ãƒã‚±ãƒƒãƒˆ: {config['bucket_name']}")
            st.sidebar.write(f"  â€¢ Lambda: {config['lambda_function_name']}")
            return config
    except Exception as e:
        st.sidebar.warning(f"aws_config.jsonèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: Streamlit Secretsã‹ã‚‰èª­ã¿è¾¼ã¿
    default_bucket = st.secrets.get('S3_BUCKET_NAME', 'excel-aggregator-backup-20250714')
    
    config = {
        'region': st.secrets.get('AWS_DEFAULT_REGION', 'ap-northeast-1'),
        'lambda_function_name': st.secrets.get('LAMBDA_FUNCTION_NAME', 'excel-data-aggregator'),
        'bucket_name': default_bucket
    }
    
    return config

def get_or_create_bucket(s3_client):
    """S3ãƒã‚±ãƒƒãƒˆã‚’å–å¾—ã¾ãŸã¯ä½œæˆ"""
    aws_config = get_aws_config()
    bucket_name = aws_config['bucket_name']
    
    try:
        # æ—¢å­˜ãƒã‚±ãƒƒãƒˆã®ç¢ºèª
        s3_client.head_bucket(Bucket=bucket_name)
        st.success(f"âœ… æ—¢å­˜ã®S3ãƒã‚±ãƒƒãƒˆã‚’ä½¿ç”¨: {bucket_name}")
        return bucket_name
    except Exception as e:
        if "403" in str(e) or "Forbidden" in str(e):
            st.warning(f"âš ï¸ ãƒã‚±ãƒƒãƒˆ {bucket_name} ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ãŒã‚ã‚Šã¾ã›ã‚“")
            
            # åˆ©ç”¨å¯èƒ½ãªãƒã‚±ãƒƒãƒˆã‚’ç¢ºèª
            try:
                response = s3_client.list_buckets()
                available_buckets = [bucket['Name'] for bucket in response['Buckets']]
                
                if available_buckets:
                    st.info("ğŸ“‹ åˆ©ç”¨å¯èƒ½ãªS3ãƒã‚±ãƒƒãƒˆ:")
                    for bucket in available_buckets[:5]:  # æœ€åˆã®5å€‹ã‚’è¡¨ç¤º
                        st.write(f"   â€¢ {bucket}")
                    
                    # æœ€åˆã®åˆ©ç”¨å¯èƒ½ãªãƒã‚±ãƒƒãƒˆã‚’ä½¿ç”¨
                    selected_bucket = available_buckets[0]
                    st.success(f"âœ… åˆ©ç”¨å¯èƒ½ãªãƒã‚±ãƒƒãƒˆã‚’ä½¿ç”¨: {selected_bucket}")
                    return selected_bucket
                else:
                    # æ–°ã—ã„ãƒã‚±ãƒƒãƒˆã‚’ä½œæˆ
                    timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
                    new_bucket_name = f"excel-aggregator-{timestamp}"
                    
                    try:
                        if aws_config['region'] == 'us-east-1':
                            s3_client.create_bucket(Bucket=new_bucket_name)
                        else:
                            s3_client.create_bucket(
                                Bucket=new_bucket_name,
                                CreateBucketConfiguration={'LocationConstraint': aws_config['region']}
                            )
                        
                        st.success(f"âœ… æ–°ã—ã„S3ãƒã‚±ãƒƒãƒˆã‚’ä½œæˆ: {new_bucket_name}")
                        return new_bucket_name
                    except Exception as create_error:
                        st.error(f"âŒ ãƒã‚±ãƒƒãƒˆä½œæˆã‚¨ãƒ©ãƒ¼: {create_error}")
                        return None
            except Exception as list_error:
                st.error(f"âŒ ãƒã‚±ãƒƒãƒˆä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {list_error}")
                return None
        else:
            st.error(f"âŒ ãƒã‚±ãƒƒãƒˆç¢ºèªã‚¨ãƒ©ãƒ¼: {e}")
            return None

@st.cache_resource
def init_aws_clients():
    """AWS ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–"""
    try:
        aws_config = get_aws_config()
        
        # Streamlit Cloudã®ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆã‹ã‚‰èªè¨¼æƒ…å ±ã‚’å–å¾—
        aws_access_key_id = st.secrets.get('AWS_ACCESS_KEY_ID')
        aws_secret_access_key = st.secrets.get('AWS_SECRET_ACCESS_KEY')
        
        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¡¨ç¤º
        st.sidebar.write("ğŸ” èªè¨¼æƒ…å ±ãƒ‡ãƒãƒƒã‚°:")
        st.sidebar.write(f"Access Key: {aws_access_key_id[:10]}..." if aws_access_key_id else "Access Key: æœªè¨­å®š")
        st.sidebar.write(f"Secret Key: {'***è¨­å®šæ¸ˆã¿***' if aws_secret_access_key else 'æœªè¨­å®š'}")
        st.sidebar.write(f"Region: {aws_config['region']}")
        
        if aws_access_key_id and aws_secret_access_key:
            session = boto3.Session(
                aws_access_key_id=aws_access_key_id,
                aws_secret_access_key=aws_secret_access_key,
                region_name=aws_config['region']
            )
        else:
            session = boto3.Session(region_name=aws_config['region'])
        
        s3_client = session.client('s3')
        lambda_client = session.client('lambda')
        
        # èªè¨¼æƒ…å ±ã®ãƒ†ã‚¹ãƒˆ
        try:
            caller_identity = session.client('sts').get_caller_identity()
            st.sidebar.write(f"ğŸ‘¤ AWSãƒ¦ãƒ¼ã‚¶ãƒ¼: {caller_identity.get('Arn', 'Unknown')}")
        except Exception as auth_error:
            st.sidebar.error(f"èªè¨¼ã‚¨ãƒ©ãƒ¼: {auth_error}")
            return None, None, False, f"èªè¨¼ã‚¨ãƒ©ãƒ¼: {auth_error}"
        
        # S3ãƒã‚±ãƒƒãƒˆã®ç¢ºèªãƒ»ä½œæˆ
        bucket_name = get_or_create_bucket(s3_client)
        if not bucket_name:
            return None, None, False, "S3ãƒã‚±ãƒƒãƒˆã®è¨­å®šã«å¤±æ•—ã—ã¾ã—ãŸ"
        
        # Lambdaé–¢æ•°ã®ç¢ºèª
        try:
            lambda_client.get_function(FunctionName=aws_config['lambda_function_name'])
        except Exception as lambda_error:
            st.sidebar.warning(f"Lambdaé–¢æ•°ã‚¨ãƒ©ãƒ¼: {lambda_error}")
            return s3_client, lambda_client, True, bucket_name  # S3ã¯æˆåŠŸã—ã¦ã„ã‚‹ã®ã§ç¶šè¡Œ
        
        return s3_client, lambda_client, True, bucket_name
        
    except Exception as e:
        st.sidebar.error(f"åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        return None, None, False, str(e)

def upload_file_to_s3(file_obj, s3_client, key, bucket_name):
    """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"""
    try:
        s3_client.upload_fileobj(file_obj, bucket_name, key)
        return True
    except Exception as e:
        st.error(f"S3ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
        return False

def invoke_lambda_function(lambda_client, payload):
    """Lambdaé–¢æ•°ã‚’å®Ÿè¡Œ"""
    try:
        aws_config = get_aws_config()
        response = lambda_client.invoke(
            FunctionName=aws_config['lambda_function_name'],
            Payload=json.dumps(payload)
        )
        
        result = json.loads(response['Payload'].read().decode())
        return result
    except Exception as e:
        st.error(f"Lambdaå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        return None

def download_file_from_s3(s3_client, key, bucket_name):
    """S3ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
    try:
        response = s3_client.get_object(Bucket=bucket_name, Key=key)
        return response['Body'].read()
    except Exception as e:
        st.error(f"S3ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def setup_sidebar():
    """ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®è¨­å®š"""
    st.sidebar.header("ğŸ“‹ ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±")
    
    # ãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±
    st.sidebar.markdown("""
    ### ğŸ¯ Excelé›†è¨ˆã‚·ã‚¹ãƒ†ãƒ 
    - **ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 1.1.0
    - **æœ€çµ‚æ›´æ–°**: 2025-07-14
    - **ç’°å¢ƒ**: Streamlit Cloud + AWS Lambda
    - **ãƒ‡ãƒ¼ã‚¿æŠ½å‡º**: Smart AIé¢¨ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
    """)
    
    # ãƒ‡ãƒãƒƒã‚°æƒ…å ±
    st.sidebar.markdown("""
    ### ğŸ” ãƒ‡ãƒãƒƒã‚°æƒ…å ±
    """)
    
    # Pythonãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±
    import sys
    st.sidebar.write(f"Python: {sys.version.split()[0]}")
    
    # pandasãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±
    try:
        import pandas as pd
        st.sidebar.write(f"Pandas: {pd.__version__}")
    except ImportError as e:
        st.sidebar.error(f"Pandas: ã‚¨ãƒ©ãƒ¼ ({str(e)[:30]}...)")
    
    # streamlitãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±
    st.sidebar.write(f"Streamlit: {st.__version__}")
    
    st.sidebar.markdown("""
    ### ğŸ“Š å¯¾å¿œé …ç›®
    - å£²ä¸Šé«˜ã€å£²ä¸ŠåŸä¾¡ã€å£²ä¸Šç·åˆ©ç›Š
    - å–¶æ¥­åˆ©ç›Šã€çµŒå¸¸åˆ©ç›Šã€å½“æœŸç´”åˆ©ç›Š
    - EBITDAã€EBIT
    """)

def show_system_stats(s3_client, bucket_name):
    """ã‚·ã‚¹ãƒ†ãƒ çµ±è¨ˆã®è¡¨ç¤º"""
    try:
        # S3ãƒã‚±ãƒƒãƒˆã®å†…å®¹ã‚’ç¢ºèª
        response = s3_client.list_objects_v2(Bucket=bucket_name)
        
        if 'Contents' in response:
            total_files = len(response['Contents'])
            total_size = sum(obj['Size'] for obj in response['Contents'])
            
            col1, col2 = st.columns(2)
            with col1:
                st.metric("ğŸ“ ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°", total_files)
            with col2:
                st.metric("ğŸ’¾ ç·ã‚µã‚¤ã‚º", f"{total_size / 1024 / 1024:.1f} MB")
            
            # æœ€è¿‘ã®å‡¦ç†çŠ¶æ³
            output_files = [obj for obj in response['Contents'] if obj['Key'].startswith('outputs/')]
            if output_files:
                latest_file = max(output_files, key=lambda x: x['LastModified'])
                st.metric("ğŸ“… æœ€æ–°å‡¦ç†", latest_file['LastModified'].strftime('%m/%d %H:%M'))
        else:
            st.info("ğŸ“­ å‡¦ç†å±¥æ­´ãªã—")
    except Exception as e:
        st.warning(f"âš ï¸ çµ±è¨ˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    
    # ãƒ˜ãƒƒãƒ€ãƒ¼
    st.title("ğŸ“Š Excel ãƒ‡ãƒ¼ã‚¿é›†è¨ˆã‚·ã‚¹ãƒ†ãƒ ")
    st.markdown("### ğŸš€ AWS Lambdaé€£æº äº‹æ¥­è¨ˆç”»ãƒ‡ãƒ¼ã‚¿è‡ªå‹•æŠ½å‡ºã‚·ã‚¹ãƒ†ãƒ ")
    st.markdown("---")
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼æƒ…å ±
    setup_sidebar()
    
    # AWSæ¥ç¶šçŠ¶æ…‹ã®ç¢ºèª
    with st.spinner("ğŸ” AWSæ¥ç¶šã‚’ç¢ºèªä¸­..."):
        s3_client, lambda_client, aws_connected, result = init_aws_clients()
    
    if not aws_connected:
        st.error("âŒ AWSæ¥ç¶šã«å¤±æ•—ã—ã¾ã—ãŸ")
        st.code(f"ã‚¨ãƒ©ãƒ¼è©³ç´°: {result}")
        show_aws_setup_guide()
        return
    
    # æ¥ç¶šæˆåŠŸæ™‚ã¯resultã¯bucket_name
    bucket_name = result
    st.success("âœ… AWSæ¥ç¶šæˆåŠŸ")
    st.info(f"ğŸª£ ä½¿ç”¨ä¸­ã®S3ãƒã‚±ãƒƒãƒˆ: {bucket_name}")
    
    # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
    col1, col2 = st.columns([3, 1])
    
    with col1:
        st.header("ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
        
        # inputç”¨ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«
        st.subheader("ğŸ¯ inputç”¨ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«")
        input_template_file = st.file_uploader(
            "inputç”¨.xlsxãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„",
            type=['xlsx'],
            key="input_template",
            help="é›†è¨ˆçµæœã‚’å…¥åŠ›ã™ã‚‹ãŸã‚ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«"
        )
        
        if input_template_file:
            st.success(f"âœ… {input_template_file.name} ãŒé¸æŠã•ã‚Œã¾ã—ãŸ")
        
        # äº‹æ¥­è¨ˆç”»ãƒ•ã‚¡ã‚¤ãƒ«
        st.subheader("ğŸ“ˆ äº‹æ¥­è¨ˆç”»ãƒ•ã‚¡ã‚¤ãƒ«")
        source_files = st.file_uploader(
            "äº‹æ¥­è¨ˆç”»Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ï¼ˆè¤‡æ•°å¯ï¼‰",
            type=['xlsx', 'xls'],
            accept_multiple_files=True,
            key="source_files",
            help="ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã™ã‚‹å…ƒã®Excelãƒ•ã‚¡ã‚¤ãƒ«"
        )
        
        if source_files:
            st.success(f"âœ… {len(source_files)} ãƒ•ã‚¡ã‚¤ãƒ«ãŒé¸æŠã•ã‚Œã¾ã—ãŸ")
            for i, file in enumerate(source_files):
                st.write(f"  {i+1}. ğŸ“„ {file.name}")
        
        # å‡¦ç†ã‚ªãƒ—ã‚·ãƒ§ãƒ³
        st.subheader("âš™ï¸ å‡¦ç†ã‚ªãƒ—ã‚·ãƒ§ãƒ³")
        col3, col4 = st.columns(2)
        
        with col3:
            show_progress = st.checkbox("å‡¦ç†çŠ¶æ³ã‚’è¡¨ç¤º", value=True)
            zip_results = st.checkbox("çµæœã‚’ZIPã§åœ§ç¸®", value=len(source_files) > 1 if source_files else False)
        
        with col4:
            auto_download = st.checkbox("è‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", value=True)
            keep_files = st.checkbox("S3ã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿æŒ", value=False)
    
    with col2:
        st.header("ğŸ“Š ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹")
        
        # AWSè¨­å®šæƒ…å ±
        aws_config = get_aws_config()
        st.info(f"âš¡ **Lambdaé–¢æ•°**\n{aws_config['lambda_function_name']}")
        st.info(f"ğŸŒ **ãƒªãƒ¼ã‚¸ãƒ§ãƒ³**\n{aws_config['region']}")
        
        # Lambdaé–¢æ•°ã®çŠ¶æ…‹ç¢ºèª
        try:
            lambda_client.get_function(FunctionName=aws_config['lambda_function_name'])
            st.success("âœ… Lambdaé–¢æ•°ç¢ºèªæ¸ˆã¿")
        except Exception as e:
            st.error(f"âŒ Lambdaé–¢æ•°ã‚¨ãƒ©ãƒ¼: {str(e)[:50]}...")
        
        # ã‚·ã‚¹ãƒ†ãƒ çµ±è¨ˆ
        show_system_stats(s3_client, bucket_name)
        
        # ãƒ˜ãƒ«ãƒ—
        with st.expander("ğŸ’¡ ä½¿ç”¨æ–¹æ³•"):
            st.markdown("""
            **åŸºæœ¬çš„ãªä½¿ç”¨æ–¹æ³•:**
            1. inputç”¨ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
            2. äº‹æ¥­è¨ˆç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
            3. "ãƒ‡ãƒ¼ã‚¿é›†è¨ˆå®Ÿè¡Œ"ã‚’ã‚¯ãƒªãƒƒã‚¯
            4. å‡¦ç†çµæœã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            
            **å¯¾å¿œãƒ•ã‚¡ã‚¤ãƒ«:**
            - Excel (.xlsx, .xls)
            - æœ€å¤§50MB
            
            **æŠ½å‡ºã•ã‚Œã‚‹é …ç›®:**
            - å£²ä¸Šé«˜ã€å£²ä¸ŠåŸä¾¡ã€å–¶æ¥­åˆ©ç›Š
            - çµŒå¸¸åˆ©ç›Šã€å½“æœŸç´”åˆ©ç›Šã€EBITDA ãªã©
            
            **AIé¢¨ã‚¹ãƒãƒ¼ãƒˆæŠ½å‡º:**
            - ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ã§æŸ”è»Ÿãªé …ç›®æ¤œå‡º
            - éš£æ¥ã‚»ãƒ«ã‹ã‚‰æ•°å€¤ã‚’è‡ªå‹•å–å¾—
            - è¤‡æ•°ã‚·ãƒ¼ãƒˆå¯¾å¿œ
            """)
    
    # å‡¦ç†å®Ÿè¡Œãƒœã‚¿ãƒ³
    st.markdown("---")
    if st.button("ğŸš€ ãƒ‡ãƒ¼ã‚¿é›†è¨ˆå®Ÿè¡Œ", type="primary", use_container_width=True):
        if not input_template_file:
            st.error("âŒ inputç”¨ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„")
        elif not source_files:
            st.error("âŒ äº‹æ¥­è¨ˆç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å°‘ãªãã¨ã‚‚1ã¤ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„")
        else:
            process_files(
                s3_client,
                lambda_client,
                bucket_name,
                input_template_file,
                source_files,
                show_progress,
                auto_download,
                zip_results,
                keep_files
            )

def process_files(s3_client, lambda_client, bucket_name, input_template_file, source_files, 
                 show_progress, auto_download, zip_results, keep_files):
    """ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã®ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    
    # å‡¦ç†IDç”Ÿæˆ
    process_id = str(uuid.uuid4())[:8]
    
    # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã¨ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    try:
        # ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ•ã‚¡ã‚¤ãƒ«ã‚’S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        status_text.text("ğŸ“¤ ãƒ•ã‚¡ã‚¤ãƒ«ã‚’S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­...")
        progress_bar.progress(0.1)
        
        # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        template_key = f"templates/{process_id}_input.xlsx"
        input_template_file.seek(0)
        if not upload_file_to_s3(input_template_file, s3_client, template_key, bucket_name):
            st.error("âŒ ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return
        
        # ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        source_keys = []
        for i, source_file in enumerate(source_files):
            source_key = f"source-files/{process_id}_{i}_{source_file.name}"
            source_file.seek(0)
            if upload_file_to_s3(source_file, s3_client, source_key, bucket_name):
                source_keys.append(source_key)
                # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã®å€¤ã‚’0.0-1.0ã®ç¯„å›²å†…ã«æ­£è¦åŒ–
                progress_value = 0.1 + (i + 1) * 0.25 / len(source_files)
                progress_bar.progress(min(progress_value, 1.0))
            else:
                st.error(f"âŒ {source_file.name} ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ")
                return
        
        # ã‚¹ãƒ†ãƒƒãƒ—2: Lambdaé–¢æ•°ã‚’å®Ÿè¡Œ
        status_text.text("âš¡ Lambdaé–¢æ•°ã‚’å®Ÿè¡Œä¸­...")
        progress_bar.progress(0.4)
        
        aws_config = get_aws_config()
        
        # Lambdaå®Ÿè¡Œç”¨ã®ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰
        payload = {
            "bucket": bucket_name,
            "input_template_key": template_key,
            "source_files": source_keys,
            "output_prefix": f"outputs/{process_id}_"
        }
        
        if show_progress:
            with st.expander("ğŸ” å®Ÿè¡Œè©³ç´°"):
                st.json(payload)
        
        # Lambdaé–¢æ•°å®Ÿè¡Œ
        lambda_result = invoke_lambda_function(lambda_client, payload)
        
        if not lambda_result:
            st.error("âŒ Lambdaé–¢æ•°ã®å®Ÿè¡Œã«å¤±æ•—ã—ã¾ã—ãŸ")
            st.info("ğŸ’¡ ãƒ’ãƒ³ãƒˆ: Lambdaé–¢æ•°ãŒæ­£ã—ããƒ‡ãƒ—ãƒ­ã‚¤ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„")
            return
        
        progress_bar.progress(0.6)
        
        # ã‚¹ãƒ†ãƒƒãƒ—3: çµæœã®å‡¦ç†
        status_text.text("ğŸ“Š çµæœã‚’å‡¦ç†ä¸­...")
        
        if lambda_result.get('statusCode') == 200:
            # pandasã‚’ã“ã“ã§ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆé…å»¶ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼‰
            try:
                import pandas as pd
            except ImportError:
                st.error("âŒ pandasãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚çµæœè¡¨ç¤ºãŒåˆ¶é™ã•ã‚Œã¾ã™ã€‚")
                pd = None
            
            body = json.loads(lambda_result['body']) if isinstance(lambda_result.get('body'), str) else lambda_result.get('body', {})
            results = body.get('results', [])
            processed_files = body.get('processed_files', [])
            
            progress_bar.progress(0.8)
            
            # çµæœè¡¨ç¤º
            st.header("ğŸ“‹ å‡¦ç†çµæœ")
            
            # Lambdaå®Ÿè¡Œçµæœã®è©³ç´°è¡¨ç¤º
            if show_progress:
                with st.expander("ğŸ” Lambdaå®Ÿè¡Œçµæœè©³ç´°"):
                    st.json(lambda_result)
            
            # çµæœãƒ†ãƒ¼ãƒ–ãƒ«
            if results:
                if pd is not None:
                    try:
                        results_df = pd.DataFrame(results)
                        st.dataframe(results_df, use_container_width=True)
                    except Exception as e:
                        st.warning(f"ãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤ºã‚¨ãƒ©ãƒ¼: {e}")
                        st.json(results)  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ã—ã¦JSONè¡¨ç¤º
                else:
                    st.json(results)  # pandasãŒãªã„å ´åˆã¯JSONè¡¨ç¤º
                
                # æˆåŠŸ/å¤±æ•—ã®çµ±è¨ˆ
                success_count = len([r for r in results if r.get('status') == 'success'])
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    st.metric("âœ… æˆåŠŸ", success_count)
                with col2:
                    st.metric("âŒ å¤±æ•—", len(results) - success_count)
                with col3:
                    st.metric("ğŸ“Š ç·æŠ½å‡ºé …ç›®", sum(r.get('extracted_items', 0) for r in results if 'extracted_items' in r))
            
            # ã‚¹ãƒ†ãƒƒãƒ—4: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            if processed_files:
                status_text.text("ğŸ“¥ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æº–å‚™ä¸­...")
                progress_bar.progress(0.9)
                
                st.header("ğŸ“¥ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
                
                download_files(s3_client, processed_files, zip_results, bucket_name)
                
                # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                if not keep_files:
                    cleanup_files(s3_client, template_key, source_keys, bucket_name)
            
            progress_bar.progress(1.0)
            status_text.text("âœ… å‡¦ç†å®Œäº†")
            
            st.success(f"ğŸ‰ å‡¦ç†å®Œäº†: {len(processed_files)} ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ­£å¸¸ã«å‡¦ç†ã•ã‚Œã¾ã—ãŸ")
            st.balloons()
        else:
            st.error(f"âŒ Lambdaå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {lambda_result}")
        
    except Exception as e:
        st.error(f"âŒ å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        with st.expander("ğŸ” ã‚¨ãƒ©ãƒ¼è©³ç´°"):
            st.exception(e)

def download_files(s3_client, processed_files, zip_results, bucket_name):
    """ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å‡¦ç†"""
    from pathlib import Path
    
    if len(processed_files) == 1:
        # å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆ
        file_key = processed_files[0]
        file_data = download_file_from_s3(s3_client, file_key, bucket_name)
        
        if file_data:
            file_name = Path(file_key).name
            st.download_button(
                label=f"ğŸ“„ {file_name} ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                data=file_data,
                file_name=file_name,
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )
    
    elif zip_results:
        # è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ZIPã§åœ§ç¸®
        zip_buffer = io.BytesIO()
        
        with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
            for file_key in processed_files:
                file_data = download_file_from_s3(s3_client, file_key, bucket_name)
                if file_data:
                    file_name = Path(file_key).name
                    zip_file.writestr(file_name, file_data)
        
        zip_buffer.seek(0)
        
        st.download_button(
            label=f"ğŸ“¦ å…¨çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’ZIPã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ ({len(processed_files)}ä»¶)",
            data=zip_buffer.getvalue(),
            file_name=f"excel_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip",
            mime="application/zip"
        )
    
    else:
        # å€‹åˆ¥ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        for i, file_key in enumerate(processed_files):
            file_data = download_file_from_s3(s3_client, file_key, bucket_name)
            if file_data:
                file_name = Path(file_key).name
                st.download_button(
                    label=f"ğŸ“„ {file_name}",
                    data=file_data,
                    file_name=file_name,
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                    key=f"download_{i}"
                )

def cleanup_files(s3_client, template_key, source_keys, bucket_name):
    """ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
    try:
        # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã¨ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
        keys_to_delete = [template_key] + source_keys
        
        for key in keys_to_delete:
            s3_client.delete_object(Bucket=bucket_name, Key=key)
        
        st.info(f"ğŸ§¹ {len(keys_to_delete)} å€‹ã®ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã—ã¾ã—ãŸ")
    except Exception as e:
        st.warning(f"âš ï¸ ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")

def show_aws_setup_guide():
    """AWSè¨­å®šã‚¬ã‚¤ãƒ‰ã®è¡¨ç¤º"""
    st.error("### âŒ AWSè¨­å®šãŒå¿…è¦ã§ã™")
    
    with st.expander("ğŸ”§ AWSè¨­å®šã‚¬ã‚¤ãƒ‰", expanded=True):
        st.markdown("""
        ### S3ãƒã‚±ãƒƒãƒˆã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ã®è¨­å®š
        
        1. **AWS Console** â†’ **IAM** â†’ **Users** â†’ ã‚ãªãŸã®ãƒ¦ãƒ¼ã‚¶ãƒ¼
        2. **Permissions** ã‚¿ãƒ– â†’ **Add permissions** â†’ **Attach policies directly**
        3. ä»¥ä¸‹ã®ãƒãƒªã‚·ãƒ¼ã‚’è¿½åŠ :
           - `AmazonS3FullAccess`
           - `AWSLambdaInvokeFunction`
        
        ### ã¾ãŸã¯æ–°ã—ã„S3ãƒã‚±ãƒƒãƒˆã‚’ä½œæˆ
        
        ```bash
        aws s3 mb s3://your-new-bucket-name --region ap-northeast-1
        ```
        
        ### Streamlit Cloud Secretsè¨­å®š
        
        ```toml
        AWS_ACCESS_KEY_ID = "your_access_key"
        AWS_SECRET_ACCESS_KEY = "your_secret_key"
        AWS_DEFAULT_REGION = "ap-northeast-1"
        S3_BUCKET_NAME = "your-new-bucket-name"
        LAMBDA_FUNCTION_NAME = "excel-data-aggregator"
        ```
        """)

if __name__ == "__main__":
    main()
